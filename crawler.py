#!/usr/bin/env python3
"""
ê²Œì„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
RSS í”¼ë“œì—ì„œ ê²Œì„ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import feedparser
from supabase import create_client, Client
from datetime import datetime
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("SUPABASE_URL and SUPABASE_KEY must be set in environment variables")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# RSS í”¼ë“œ ëª©ë¡ (í•œêµ­ + ì˜ë¬¸ ê²Œì„ ë‰´ìŠ¤)
RSS_FEEDS = [
    # í•œêµ­ ê²Œì„ ë‰´ìŠ¤ (Google News)
    {
        "url": "https://news.google.com/rss/search?q=ê²Œì„ì‚°ì—…+OR+ë„¥ìŠ¨+OR+ì—”ì”¨ì†Œí”„íŠ¸+OR+í¬ë˜í”„í†¤+when:1d&hl=ko&gl=KR&ceid=KR:ko",
        "category": "Industry",
        "name": "ê²Œì„ ì‚°ì—… (Industry)"
    },
    {
        "url": "https://news.google.com/rss/search?q=ê²Œì„ê°œë°œ+OR+ì–¸ë¦¬ì–¼ì—”ì§„+OR+ì¸ë””ê²Œì„+when:1d&hl=ko&gl=KR&ceid=KR:ko",
        "category": "Dev",
        "name": "ê²Œì„ ê°œë°œ (Dev)"
    },
    # ì˜ë¬¸ ê²Œì„ ë‰´ìŠ¤
    {
        "url": "https://www.gamedeveloper.com/rss.xml",
        "category": "Game Developer",
        "name": "Game Developer"
    },
    {
        "url": "https://www.gamesindustry.biz/feed",
        "category": "GamesIndustry.biz",
        "name": "GamesIndustry.biz"
    },
    {
        "url": "https://www.polygon.com/rss/index.xml",
        "category": "Polygon",
        "name": "Polygon"
    },
]

def clean_summary(text: str, max_length: int = 300) -> str:
    """ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤."""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ë°©ë²•)
    import re
    text = re.sub(r'<[^>]+>', '', text)
    
    # ê³µë°± ì •ë¦¬
    text = ' '.join(text.split())
    
    # ê¸¸ì´ ì œí•œ
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    return text

def fetch_and_store_news():
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    total_added = 0
    total_skipped = 0
    
    print(f"ğŸš€ Starting news crawler at {datetime.now()}")
    
    for feed_info in RSS_FEEDS:
        print(f"\nğŸ“° Fetching from {feed_info['name']}...")
        
        try:
            feed = feedparser.parse(feed_info['url'])
            
            if feed.bozo:
                print(f"âš ï¸  Warning: Feed parsing error for {feed_info['name']}")
            
            for entry in feed.entries[:10]:  # ìµœê·¼ 10ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
                try:
                    title = entry.get('title', 'No Title')
                    link = entry.get('link', '')
                    summary = clean_summary(entry.get('summary', entry.get('description', '')))
                    
                    if not link:
                        print(f"  â­ï¸  Skipping entry without link: {title}")
                        continue
                    
                    # ì¹´í…Œê³ ë¦¬ëŠ” RSS í”¼ë“œ ì¶œì²˜ ì‚¬ìš©
                    category = feed_info['category']
                    
                    # ì¤‘ë³µ í™•ì¸ (ê°™ì€ ë§í¬ê°€ ì´ë¯¸ ìˆëŠ”ì§€)
                    existing = supabase.table('posts').select('id').eq('original_link', link).execute()
                    
                    if existing.data:
                        print(f"  â­ï¸  Already exists: {title[:50]}...")
                        total_skipped += 1
                        continue
                    
                    # Supabaseì— ì €ì¥
                    data = {
                        'title': title,
                        'summary': summary or 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.',
                        'original_link': link,
                        'category': category,
                    }
                    
                    result = supabase.table('posts').insert(data).execute()
                    
                    if result.data:
                        print(f"  âœ… Added: {title[:50]}... [{category}]")
                        total_added += 1
                    else:
                        print(f"  âŒ Failed to add: {title[:50]}...")
                        
                except Exception as e:
                    print(f"  âŒ Error processing entry: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"âŒ Error fetching feed {feed_info['name']}: {str(e)}")
            continue
    
    print(f"\nâœ¨ Crawler finished!")
    print(f"ğŸ“Š Summary: {total_added} added, {total_skipped} skipped")
    
    return total_added, total_skipped

if __name__ == "__main__":
    try:
        added, skipped = fetch_and_store_news()
        print(f"\nğŸ‰ Success! Added {added} new posts.")
    except Exception as e:
        print(f"\nğŸ’¥ Fatal error: {str(e)}")
        exit(1)
