#!/usr/bin/env python3
"""
ê²Œì„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
RSS í”¼ë“œì—ì„œ ê²Œì„ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import feedparser
from supabase import create_client, Client
from datetime import datetime
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("SUPABASE_URL and SUPABASE_KEY must be set in environment variables")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# RSS í”¼ë“œ ëª©ë¡ (ê²Œì„ ê´€ë ¨ ë‰´ìŠ¤ ì‚¬ì´íŠ¸)
RSS_FEEDS = [
    {
        "url": "https://www.gamedeveloper.com/rss.xml",
        "category": "Dev",
        "name": "Game Developer"
    },
    {
        "url": "https://www.gamesindustry.biz/feed",
        "category": "Business",
        "name": "GamesIndustry.biz"
    },
    {
        "url": "https://www.polygon.com/rss/index.xml",
        "category": "Tech",
        "name": "Polygon"
    },
]

def clean_summary(text: str, max_length: int = 300) -> str:
    """ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤."""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ë°©ë²•)
    import re
    text = re.sub(r'<[^>]+>', '', text)
    
    # ê³µë°± ì •ë¦¬
    text = ' '.join(text.split())
    
    # ê¸¸ì´ ì œí•œ
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    return text

def categorize_entry(entry: dict, default_category: str) -> str:
    """ë‰´ìŠ¤ í•­ëª©ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    title = entry.get('title', '').lower()
    summary = entry.get('summary', '').lower()
    content = title + ' ' + summary
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    if any(word in content for word in ['esports', 'tournament', 'championship', 'league']):
        return 'Esports'
    elif any(word in content for word in ['release', 'launch', 'announced', 'reveal']):
        return 'Release'
    elif any(word in content for word in ['unity', 'unreal', 'engine', 'tool', 'sdk', 'api']):
        return 'Tech'
    elif any(word in content for word in ['business', 'revenue', 'sales', 'market', 'investment']):
        return 'Business'
    else:
        return default_category

def fetch_and_store_news():
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    total_added = 0
    total_skipped = 0
    
    print(f"ğŸš€ Starting news crawler at {datetime.now()}")
    
    for feed_info in RSS_FEEDS:
        print(f"\nğŸ“° Fetching from {feed_info['name']}...")
        
        try:
            feed = feedparser.parse(feed_info['url'])
            
            if feed.bozo:
                print(f"âš ï¸  Warning: Feed parsing error for {feed_info['name']}")
            
            for entry in feed.entries[:10]:  # ìµœê·¼ 10ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
                try:
                    title = entry.get('title', 'No Title')
                    link = entry.get('link', '')
                    summary = clean_summary(entry.get('summary', entry.get('description', '')))
                    
                    if not link:
                        print(f"  â­ï¸  Skipping entry without link: {title}")
                        continue
                    
                    # ì¹´í…Œê³ ë¦¬ ê²°ì •
                    category = categorize_entry(entry, feed_info['category'])
                    
                    # ì¤‘ë³µ í™•ì¸ (ê°™ì€ ë§í¬ê°€ ì´ë¯¸ ìˆëŠ”ì§€)
                    existing = supabase.table('posts').select('id').eq('original_link', link).execute()
                    
                    if existing.data:
                        print(f"  â­ï¸  Already exists: {title[:50]}...")
                        total_skipped += 1
                        continue
                    
                    # Supabaseì— ì €ì¥
                    data = {
                        'title': title,
                        'summary': summary or 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.',
                        'original_link': link,
                        'category': category,
                    }
                    
                    result = supabase.table('posts').insert(data).execute()
                    
                    if result.data:
                        print(f"  âœ… Added: {title[:50]}... [{category}]")
                        total_added += 1
                    else:
                        print(f"  âŒ Failed to add: {title[:50]}...")
                        
                except Exception as e:
                    print(f"  âŒ Error processing entry: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"âŒ Error fetching feed {feed_info['name']}: {str(e)}")
            continue
    
    print(f"\nâœ¨ Crawler finished!")
    print(f"ğŸ“Š Summary: {total_added} added, {total_skipped} skipped")
    
    return total_added, total_skipped

if __name__ == "__main__":
    try:
        added, skipped = fetch_and_store_news()
        print(f"\nğŸ‰ Success! Added {added} new posts.")
    except Exception as e:
        print(f"\nğŸ’¥ Fatal error: {str(e)}")
        exit(1)
